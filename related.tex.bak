\section{Related Works} \label{rel}
\noindent
Several techniques have been proposed and adopted in real time predictable DRAM controllers. 
In normal systems, multiple requesters generate memory requests to the DRAM controller, which finally schedules the requests 
to the DRAM for processing the requests. 
In commercial off-the-shelf (COTS) 
DRAM controllers, scheduling techniques are generally applied at the software level. In custom memory controller, either 
techniques focus mainly on scheduling the memory requests or controlling the commands. So based on our requirements, we need 
to decide on the suitable address mapping scheme and page policy to be used. 
In COTS multicore systems, DRAM banks can be accessed independently. 
In~\cite{yun2014palloc}, PALLOC, a DRAM bank aware memory controller has been proposed. 
These memory controllers exploit the page-based virtual memory system to avoid bank 
sharing among cores, thereby improving isolation on COTS multicore platforms without requiring any special hardware support.
In~\cite{kim2014bounding}, techniques have been proposed to provide a tight upper bound on the worst-case memory interference
in COTS multi-core systems, where a task running on one core may get delayed by other task running on the other core due to 
shared resources. 
They have explicitly modeled the major resources in the DRAM system and considered timing characteristics by 
analyzing worst-case interference delay imposed by a parallelly running task on the other. 
A predictable DRAM controller 
design has been proposed in PRET~\cite{reineke2011pret}, where DRAM is considered as multiple 
resources that can be shared between one or more requests individually by interleaving accesses to blocks of DRAM. 
Thus contention for shared resources is eliminated within the device, making accesses temporally predictable and temporally 
isolated. Each memory request is scheduled by Time Division Multiplexing (TDM) and each DRAM commandâ€™s latency is predefined, 
so each request is isolated and tightly bounded. 
In~\cite{akesson2011architectures}, bank interleaving and a close page policy are 
used with a pre-defined command sequence. Different scheduling approaches such as dynamic scheduling can be used for systems 
where we have varying memory request patterns. But request isolation is not guaranted. 
Again in~\cite{akesson2008real}, a 
Credit-Controlled Static-Priority (CCSP) has been suggested to provide minimum bandwidth for each request with bounded 
latencies. 
In~\cite{paolieri2013timing}, an analytical model that computes worst case delay, more specifically known as Upper Bound 
Delay (UBD) has been computed considering all memory interferences generated by co-running tasks. Another approach has been 
proposed in~\cite{goossens2013reconfigurable} where a method for composable service to memory clients by composable memory 
patterns has been designed. 
A reconfigurable TDM, which can be changed at run time along with a reconfigurable protocol has 
been developed, whereas, predictable and composable performance is also offered to active memory clients which remain 
unaffected irrespective of configuration. Each request is isolated from memory interference if each task is allocated 
a slot in the TDM. But, a lot of slots are wasted if no memory request occurs resulting in decrease in throughput. 
In~\cite{hassan2017predictable}, memory requests are scheduled using time-division-multiplexing scheduler and a framework 
has been developed to statically analyse the tasks to meet the timing requirements of all tasks. 
This work proposes a
mixed page policy that dynamically switches between close and open-page policies based on the request size to combine the 
benefits of both policies while avoiding their drawbacks. 
But in this method, many slots remain unused as requests are served in an interleaved and TDM manner. 
In this paper, we have introduced a new DRAM scheduling policy which schedules the memory requests based on a cost function
based on some task parameters and schedules the memory requests so that the number of deadline misses eventually gets minimized.
